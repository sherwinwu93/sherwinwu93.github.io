Okay, here's where we are so far. We started off with a discussion of what
morality is, gave some examples of it, talked about the
scope of morality. And then we introduced the distinction
between the philosophical question of how we should be, what, what's the
right way to live, and the psychological question of
our moral reasoning and moral action, how we actually do live, how
we do think. We, we talked about philosophical theories, and we introduced a distinction
between consequentialism and deontology: roughly the theory that all that matters
is the consequences of an action versus the theory that there
are sort of these abstract principles that apply
above and beyond consequences.

Now, many experimental psychologists and
developmental psychologists and social psychologists have been very
excited. At the idea of framing the psychological question in terms of they philosophical
question. So, one way to think about people is that we're, sort of, common sense moral
philosophers. We, we think about morality. We, we have a sense of morality. We have a, a unconscience, perhaps, an unconscience theory of what's right and
wrong.

And then the question is, what kind of
moral philosophers are we? There's been a lot of research asking the
question, are people, in their gut, consequentialists or are people deontologic, deontological
thinkers? Are they Kantians? And one way to explore this, and this has
launched dozens, hundreds of experiments, is in terms of a
famous thought experiment.

This thought experiment is known as the
Trolly Problem. The Trolly Problem was developed in its initial form by the philosopher Philipa
Foot and then was expanded and developed by many other philosophers and in particular
Judith Jarvis Thompson. When I was a graduate student at MIT, she
was a professor there and I actually took one of
her Philosophy courses. Which was just terrific though I did drop
it after the second class. But that was me and not her.
So here's the trolly problem. There is a run away trolly, a run away
train. And if it is left on its, to do what it's going to do, it will go on the track to
the left. And it will kill five people. These are workers.
They're, they're banging on it. Maybe they have headphones on. They can't see the train coming. They are too, you are the person at the
switch. These people on the tracks are too far
away to warn. If you do nothing, they will die. The train will come and it will run them
over. You, however, are standing next to a
switch. If you pull that switch, the train will be
diverted and will go on the other track. Unfortunately, there is one other worker
there, also too far away to warn and that worker will be
killed.
And the question, this is a question that
has been asked, I would bet has been asked to over 100,000 people in various experiments and surveys and online
studies is, should you throw the switch? Should you, not what would you do, not,
they don't ask people ever to do it, but should you throw
the switch? What's the right thing to do? So, we call that the switch case.
Think about your answer to that. And now compare it to another case.

This is sometimes called the Footbridge
Case. Same situation to some extent. There's a runaway train. Unless something happens, that train will
proceed and kill five people, and there's not a thing you can do
to warn them. But you are standing on a bridge, and next
to you is a man. And he's, and, and, and, and if you push
him, he will plummet down, land on the tracks, and
stop the train. Sometimes this is called a fat man case. Because the idea is that people always ask
why shouldn't you jump yourself? So, the example is such that he's a big
enough guy that he'll stop the train. You're not big enough to stop the train. So, now your question is should you push
the man? Should you let the train proceed, kill
five people or should you push the man, which would kill him but
save those five. As I said, countless people have been
asked these questions. If these questions have been asked, for young children, they have been asked people from different cultures in the
world. They've been asked of criminal
psychopaths. They've been asked of people with autism, with schizophrenia, with various forms of
brain damage.

And when you look at the data for normal
people there's a striking pattern. And I would bet that this comes up in your
intuitions, too. Most people think it's okay to throw the
switch for the switch case. Most people think that, that maybe, you
know, it's not 100% you throw the switch it's the right
thing to do. You save five and one die. But most people think it's wrong to push
the man. Most people think you shouldn't do that. So, what does this tell us?
Well, the people that do this research will, will tell you that, one thing it suggests is that we're not
consequentialists. because notice these cases are, kind of
similar. The consequences are the same. If you do nothing, five people will die.
If you act, one person will die. So, if all we were doing was making our
decision based on the consequences of our acts, these two cases would elicit the identical
intuition. But they don’t. There seems to be a critical difference
between them.

And so, what could this difference be?
Well, there’s different theories of this. One theory is that we have in our brains,
unconsciously usually, a rather subtle philosophical principle and this
principle is sometimes called the Doctrine of Double
Effect. And it was it was developed, among others,
by, Thomas Aquinas. And the Doctrine of Double Effect says
there's a distinction between doing something bad, like killing or harming
somebody as an unintended consequence of causing greater good to happen, and
that could be the right thing to do, versus doing
something bad, like killing or harming somebody in
order to bring about a greater good, and that you
shouldn't do. The consequences are identical. But the difference is that in one case,
the bad thing is a regrettable byproduct. I'm sorry, in the good case, the bad thing is a
regrettable byproduct. That's what you, that's okay. Well, in the bad case, it's an instrument
through which you act. So, this often comes up, in case, cases of
just war. Or, or, or in philosophical debates and
political debates over, over what's permissible in
war time.

And people who talk about a Doctrine of
Double Effects say, consider two examples. In one example there's a munition's factory, weapon's factory. And you have to decide whether or not to
bomb it. And if you bomb it, there's good reason to
believe it would, defeat the enemy and the war would come to an end and
millions of lives would be saved. But if you bomb it, there's people who
work in the factory who are innocent, and they
will die. Should you bomb it?
Well, that's a hard question. But many people would argue if the benefit is great
enough, having some innocent people die as a
byproduct is permissible. Now compare to another case, there
innocent people and if you blow them up, if you kill them, that
will terrorize the enemy population. They'll know you're serious and then the
war will end, saving millions of lives and so
on. In fact, you can make these case so that they have identical
consequences so that the same number of people die in the same
way. But still there's intuition that these
cases are different. And the Doctrine of Double Effect says,
the difference is, in the case where you may do it, the innocents are collateral
damage, but you don't want them to die. They're just a regrettable byproduct. But here, you're killing people in order
to bring about an effect and you shouldn't do that.

And the connection to trolley problem, is
pretty clear. this, this could explain, some people
believe, why we think so differently about these two
cases. In the switch case, one person dies but
their death is an accident and their death does
nothing. If they were to leave the track, you'd be
happy. They don't have to die and it's wonderful. In the bridge case, the person's death is necessary. If they were to leave the bridge, it
doesn't work any more, the five will die. You need them. And this difference, people argue,
explains our intuition. Unwise permissible in a switch case, but
not in a bridge case.

I will add, there's all sorts of philosophical dilemmas that are so
fanciful and weird and crazy that they would never
happen in a real world, but this actually is a case where these things
happen in a real world. in, in, 9/11 the U.S. government had to
make a decision over whether or not to shoot down a plane that
was headed for the Pentagon and, and I think the German
government had a similar case a few years ago, where they had to, where
they debated whether to do this.
And this is a trolley problem. Is it permissible to kill some people in
order to save a greater amount of people? Or imagine you're driving your car home
and imagine it's a cold winter's day on ice. And you're driving a bit too fast, and you
shoot through a stop sign, and you're headed
right towards five people. And they're standing in front of you
[NOISE] and if you do nothing your car will go into them and
kill them. But you can swerve, you could swerve and
your car will spin, but there's one person over
there. You are living in a trolley problem. And you might think, well, those things
are so rare we never have to deal with them. But people pointed out that solving the
trolly problem, figuring out the right thing to do in these cases, is a matter of
real world relevance. For one thing, as Gary Marcus writes in an
interesting discussion, people are now building self driving cars.
Google's building self driving cars. And self driving cars may have to one day deal with a variants of the trolly
problem, and solve.

So, this way of looking at things where we take these abstract philosophical
theories, we use the philosophical examples, we test people
on them, and then we try to explain people's
intuitions based on, on these philosophical approaches, is
one way to do moral psychhology. And it's been a very influential approach. It treats people as moral philosophers and
uses abstract philosophical theories as a way to make sense of our deepest
intuitions and judgements. But it's not the only way to do moral
psychology.

And in fact, I think its fair to say that in the last decade or so there's been
a backlash. And in this backlash people have said this is entirely the wrong way to study moral
psychology. We should not think of people as, as these
philosophical creatures doing these abstract rules, rather moral, morality is
driven to a large extent by gut feelings. And and this is a view I want to talk
about in the lectures that, that follow. It was actually nicely summarized, I think, by, the cultural commentator and
public intellectual, David Brooks, in a New York
Times article of a few years ago. And the article was titled, The End of
Philosophy. So Brooks writes this. Think of what happens when you put a new
food into your mouth. You don't have to decide if it's
disgusting. You just know.
You don't have to decide if a landscape is beautiful.
You just know. Moral judgments are like that. They are rapid intuitive decisions and involve the emotional-processing parts of
the brain. Most of us make snap moral judgments about
what feels fair or not or what feels good or
not. We start doing this when we are babies,
before we have language, and even as adults, we don't, we often can't explain to ourselves why something feels
wrong. And, a lot of what he talks about, about the brain, and about babies,
we're going to talk about in this course and talk about
edvidence underlying his claims, which I think are for the most
part correct from this perspective, we're not these
rational, moral philosophers at all. We, we, a lot of our morality is driven by
our gut. And in fact, David Brooks then wrote a
best selling book, The Social Animal, that
expanded and elaborated on this theory and it's one I highly
recommend. In his work, David Brooks drew, to a large extent, on the work of the philosopher
Jonathan Haidt. And Jonathan Haidt wrote, many years ago,
a very influential article in Psychological Review, one of
our top journals, called The Emotional Dog and Its Rational Tail: A
Social Intuitionist Approach to Moral Judgment And you can see his
argument from the title. The idea is that if we think reason is important,
we are mistaking the tail for the dog. It's, it's the motion that counts. Haidt writes, moral judge, reasoning does not cause moral judgment; rather moral
reasoning is usually a post hoc construction generated after a judgment has been
reached. And Haidt has recently written a book summarizing his view, called The Righteous
Mind. This is the American cover. But I think the British cover better conveys the audacious nature of this
claim. In all of this work scholars like Brooks
and Haidt and the many psychologists and philosophers who endorse this emotional
approach to morality are drawing upon an important
philosophical tradition. And it's actually nicely summarized by
David Hume. So, David Hume talking about moral reasoning says, look,
our moral decisions, our moral understanding is not
driven by reason. Rather, he says, reason is, and ought only
to be, the slave of the passions. Now, so far I have not given evidence for
this. I just made the abstract argument for it. What I want to do in the next three
lectures, is give three case studies of how our gut
feelings, how our emotional responses influence, in a
profound way, our sense of right and wrong. [MUSIC]
